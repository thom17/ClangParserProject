{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97c5a023-bc6d-47fe-97f4-4eb3c2a3fa67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n2024-11-26\\n생성 목적.\\n1. OMS나 ClangCursor를 통한 태스트 데이터 셋 생성\\n2. LongCoder 재 확인.\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\"\"\"\n",
    "2024-11-26\n",
    "생성 목적.\n",
    "1. OMS나 ClangCursor를 통한 태스트 데이터 셋 생성\n",
    "2. LongCoder 재 확인.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1c97477-2527-456f-a9a8-d66f02538664",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping (100.00%) 101/101\n",
      "Mapping done\n",
      "Update Call (100.00%) 100/100\n",
      "Update done\n",
      "project parse time : 55.86138319969177 seconds\n",
      "820  clang src map size\n",
      "InfoSet(cls:78, fun:254, var:164)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "from oms.Mapper import parsing\n",
    "#일단 태스트 용도로 간단하게 하나의 파일로. 또 태스트 데이터 생성을 위해 완전한(프로젝트) 파일로.\n",
    "file_path = r'D:\\dev\\AutoPlanning\\trunk\\AP_trunk_pure\\mod_APImplantSimulation\\UIDlgImplantLib.cpp'\n",
    "st = time.time()\n",
    "oms_info, clang_src_map = parsing(file_path)\n",
    "ed = time.time()\n",
    "\n",
    "print(\"project parse time :\", (ed-st), \"seconds\")\n",
    "\n",
    "print(len(clang_src_map), ' clang src map size')\n",
    "print(oms_info)\n",
    "\n",
    "#unit도 필요한듯\n",
    "from clangParser.CUnit import CUnit\n",
    "my_unit = CUnit.parse(file_path=file_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9174681-3d8c-4c17-9ad1-9fae9f6c0d54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InfoSet(cls:0, fun:99, var:41)\n",
      "-> 46\n",
      "65 - 65\n",
      "455 - 461\n",
      "bool CUIDlgImplantLib::OnMessageSpecificAction(UINT nAction, LPARAM lParam) \t CUIDlgImplantLib ..LPARAM .UINT CUIDlgImplantLib.OnMessageSpecificAction(UINT, LPARAM).lParam CUIDlgImplantLib.OnMessageSpecificAction(UINT, LPARAM) CUIDlgImplantLib.OnMessageSpecificAction(UINT, LPARAM).nAction \n",
      "{ \t \n",
      "\t// nothing to do, now. \t \n",
      "\tUNREFERENCED_PARAMETER(nAction); \t CUIDlgImplantLib.OnMessageSpecificAction(UINT, LPARAM).nAction \n",
      "\tUNREFERENCED_PARAMETER(lParam); \t CUIDlgImplantLib.OnMessageSpecificAction(UINT, LPARAM).lParam \n",
      "\treturn false; \t \n",
      "1880 - 1893\n",
      "CString CUIDlgImplantLib::_getSystem() \t CUIDlgImplantLib CUIDlgImplantLib._getSystem() CString \n",
      "{ \t \n",
      "\tCString strSystem; \t CString CUIDlgImplantLib._getSystem().strSystem \n",
      "\tif (DCS_PRESSED == _btnTabFixture.GetState()) \t E_DOCTRL_STATES.DCS_PRESSED CUIDlgImplantLib._btnTabFixture \n",
      "\t{ \t \n",
      "\t\tstrSystem = _strFixtureSystem; \t CUIDlgImplantLib._strFixtureSystem CUIDlgImplantLib._getSystem().strSystem \n",
      "\t} \t \n",
      "\telse if (DCS_PRESSED == _btnTabAbutment.GetState()) \t CUIDlgImplantLib._btnTabAbutment E_DOCTRL_STATES.DCS_PRESSED \n",
      "\t{ \t \n",
      "\t\tstrSystem = _strAbutmentSystem; \t CUIDlgImplantLib._strAbutmentSystem CUIDlgImplantLib._getSystem().strSystem \n",
      "\t} \t \n",
      " \t \n",
      "\treturn strSystem; \t CUIDlgImplantLib._getSystem().strSystem \n",
      "3649 - 3652\n",
      "CSize CUIDlgImplantLib::GetSize() \t CUIDlgImplantLib CUIDlgImplantLib.GetSize() CSize \n",
      "{ \t \n",
      "\treturn CSize(LIB_X, LIB_Y); \t CSize.CSize(int, int) CSize \n"
     ]
    }
   ],
   "source": [
    "#적당히 짧은 메서드들만 \n",
    "from oms.dataset.info_base import InfoBase\n",
    "from clangParser.Cursor import Cursor\n",
    "short_method_list = []\n",
    "all_method_list = []\n",
    "gen_src_key_list = []\n",
    "\n",
    "def get_gen_src_key(info: InfoBase):\n",
    "    clang_list = clang_src_map[info.src_name]\n",
    "    call_trace=Cursor(clang_list[0]).get_line_call_trace()\n",
    "    codes = ''\n",
    "    for code, src_set in call_trace:\n",
    "        for src in src_set:\n",
    "            codes += src + ' '\n",
    "        codes += '\\n'\n",
    "    # print([node.location.file for node in clang_list])\n",
    "    return codes\n",
    "\n",
    "\n",
    "cls_info:InfoBase = oms_info.get_class_info('CUIDlgImplantLib')\n",
    "for fun_info in cls_info.relationInfo.hasInfoMap.functionInfos.values():\n",
    "    all_method_list.append(fun_info)\n",
    "    if 15  < len(fun_info.code.splitlines()):\n",
    "        continue\n",
    "    else:\n",
    "        short_method_list.append(fun_info)\n",
    "\n",
    "#설정된 메서드 부분 출력\n",
    "print(cls_info.relationInfo.hasInfoMap)\n",
    "print('->' , len(short_method_list))\n",
    "for idx, fun in enumerate(short_method_list):\n",
    "    if idx %15 == 0:\n",
    "        codes = fun.code.splitlines()\n",
    "        srcs = get_gen_src_key(fun).splitlines()\n",
    "        for idx, src in enumerate(srcs):\n",
    "            print(codes[idx], '\\t', src)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaebb0e-7ebb-4960-aea1-db5a49a8970a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acfd2860-62b3-408a-b3bb-477b1deb73d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/microsoft/CodeBERT/master/UniXcoder/unixcoder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "882f02b5-d12d-48a7-a68b-62177112eafe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32100, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32100, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32100, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32100, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#일단 t5로 먼저 학습\n",
    "from transformers import RobertaTokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-small')\n",
    "model.to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6d663bf-f268-4b56-a4b3-8949b5d3c83b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 - 65\n",
      "71 - 79\n",
      "84 - 95\n",
      "100 - 103\n",
      "108 - 108\n",
      "108 - 118\n",
      "169 - 175\n",
      "177 - 182\n",
      "276 - 281\n",
      "283 - 295\n",
      "399 - 404\n",
      "409 - 417\n",
      "422 - 428\n",
      "433 - 439\n",
      "444 - 450\n",
      "455 - 461\n",
      "466 - 472\n",
      "477 - 483\n",
      "492 - 501\n",
      "505 - 511\n",
      "513 - 522\n",
      "525 - 532\n",
      "536 - 544\n",
      "549 - 559\n",
      "798 - 806\n",
      "808 - 818\n",
      "1823 - 1837\n",
      "1858 - 1861\n",
      "1863 - 1866\n",
      "1868 - 1878\n",
      "1880 - 1893\n",
      "2502 - 2516\n",
      "2741 - 2751\n",
      "2753 - 2756\n",
      "2758 - 2761\n",
      "2763 - 2766\n",
      "2768 - 2771\n",
      "2773 - 2776\n",
      "2778 - 2781\n",
      "2783 - 2786\n",
      "2788 - 2791\n",
      "2793 - 2796\n",
      "3069 - 3076\n",
      "3417 - 3426\n",
      "3644 - 3647\n",
      "3649 - 3652\n"
     ]
    }
   ],
   "source": [
    "all_code_list = [fun_info.code for fun_info in short_method_list]\n",
    "all_ans_list = [get_gen_src_key(fun_info) for fun_info in short_method_list]\n",
    "# all_tokens_list = [tokenizer.tokenize(code_snippet) for code_snippet in all_code_list]\n",
    "# all_tokens_ids_list = [torch.tensor(tokenizer.convert_tokens_to_ids(tokens)) for tokens in all_tokens_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36df7635-d546-4179-977f-f8ac2c923b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([46, 258])\n",
      "torch.Size([46, 512])\n"
     ]
    }
   ],
   "source": [
    "batch_input = tokenizer(all_code_list, padding=True, truncation=True, return_tensors='pt')\n",
    "batch_ans= tokenizer(all_ans_list, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "print(batch_input[\"input_ids\"].shape)  # 입력 토큰의 ID\n",
    "print(batch_ans[\"input_ids\"].shape)  # 출력 토큰의 ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e1168c0-9682-443f-ad76-8dabf0dceb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    }
   ],
   "source": [
    "# #토큰화 된 데이터 재 출력\n",
    "# for idx, tk in enumerate(batch_input['input_ids'][30]):\n",
    "#     print(idx,\":\",tokenizer.decode(tk))\n",
    "\n",
    "print(type(batch_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bc27090-c599-45cd-85cd-30105e7aee66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3a56d2b-919e-4765-b02d-2ecdb437efa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 41\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#dataset 생성\n",
    "from datasets import Dataset\n",
    "\n",
    "#먼저 dict으로\n",
    "data_dict = {\n",
    "    \"input_ids\": batch_input[\"input_ids\"],\n",
    "    \"attention_mask\": batch_input[\"attention_mask\"],\n",
    "    \"labels\": batch_ans[\"input_ids\"]\n",
    "}\n",
    "\n",
    "# Dataset 생성\n",
    "dataset = Dataset.from_dict(data_dict)\n",
    "test_1_datas = dataset.train_test_split(test_size = 0.1, seed = 0)\n",
    "print(test_1_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ecc3ab58-0c12-45e0-9db0-8cdae33f1f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>CUIDlgImplantLib CUIDlgImplantLib.~CUIDlgImplantLib() \n",
      "\n",
      "CUIDlgImplantLib._pImgTitle \n",
      "\n",
      "CUIDlgImplantLib._pListManufacture \n",
      "CUIDlgImplantLib._pListSystem \n",
      "CUIDlgImplantLib._pListAbutment \n",
      "\n",
      "CUIDlgImplantLib._pListModel \n",
      "\n",
      "CUIDlgImplantLib._pDlgPreview \n",
      "</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "# print(test_1_datas['test'][0])\n",
    "print(tokenizer.decode(test_1_datas['test'][0]['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a72a895e-a978-4888-a3cb-f2923846d8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.46.3\n"
     ]
    }
   ],
   "source": [
    "# pip show transformers\n",
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56a56ba2-dec7-409a-bba7-7076163a5196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 41\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_1_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97177d7f-b275-4dff-a9d2-3dc42527f08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\dev\\Anaconda\\envs\\T1\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:50, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.979421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.233285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.349738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.345459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.340202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=30, training_loss=0.8403551737467448, metrics={'train_runtime': 51.6491, 'train_samples_per_second': 3.969, 'train_steps_per_second': 0.581, 'total_flos': 13980913827840.0, 'train_loss': 0.8403551737467448, 'epoch': 5.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# Seq2SeqTrainingArguments 설정\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",                   # 결과 저장 경로\n",
    "    evaluation_strategy=\"epoch\",             # 평가 빈도 (매 epoch마다)\n",
    "    learning_rate=5e-5,                       # 학습률\n",
    "    per_device_train_batch_size=8,            # 학습 배치 크기\n",
    "    per_device_eval_batch_size=8,             # 평가 배치 크기\n",
    "    num_train_epochs=5,                       # 학습 에포크 수\n",
    "    weight_decay=0.01,                        # 가중치 감소 (정규화)\n",
    "    save_total_limit=2,                       # 저장할 체크포인트 개수 제한\n",
    "    save_strategy=\"epoch\",                    # 체크포인트 저장 전략\n",
    "    logging_dir=\"./logs\",                     # 로깅 경로\n",
    "    logging_steps=100,                        # 로깅 빈도\n",
    "    predict_with_generate=True,               # 생성 기반 평가 활성화\n",
    "    generation_max_length=128,                # 생성 최대 길이\n",
    "    generation_num_beams=5                    # 빔 서치 빔 개수\n",
    ")\n",
    "\n",
    "# Trainer 초기화\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=test_1_datas[\"train\"],\n",
    "    eval_dataset=test_1_datas[\"test\"]\n",
    ")\n",
    "\n",
    "# 학습 시작\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a390c28b-aae2-4f6c-b65c-9f6b842b9059",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "(array([[[ 1.55565901e+01,  3.06917591e+01,  4.65232849e+00, ...,\n",
      "          2.41248035e+00,  2.31740689e+00,  7.27300787e+00],\n",
      "        [ 1.23878920e+00,  9.43407917e+00,  9.51668262e+00, ...,\n",
      "          1.06006565e+01,  1.08723860e+01,  2.37797527e+01],\n",
      "        [-2.06121325e-01,  4.58405876e+00,  4.56489277e+00, ...,\n",
      "          3.77803397e+00,  4.04435921e+00,  2.59056163e+00],\n",
      "        ...,\n",
      "        [ 2.65522118e+01,  6.12792492e+00,  3.57215452e+00, ...,\n",
      "          1.68595576e+00,  2.91011715e+00,  1.52811849e+00],\n",
      "        [ 2.65503445e+01,  6.12779093e+00,  3.57224011e+00, ...,\n",
      "          1.68658459e+00,  2.91057515e+00,  1.52854395e+00],\n",
      "        [ 2.65484486e+01,  6.12765503e+00,  3.57231736e+00, ...,\n",
      "          1.68720424e+00,  2.91101456e+00,  1.52895439e+00]],\n",
      "\n",
      "       [[ 1.55934525e+01,  3.04603577e+01,  4.58670855e+00, ...,\n",
      "          2.32590532e+00,  2.23005128e+00,  7.16310072e+00],\n",
      "        [ 1.12821817e+00,  9.24745846e+00,  9.35002136e+00, ...,\n",
      "          1.06320906e+01,  1.08318577e+01,  2.45869331e+01],\n",
      "        [ 2.45562240e-01,  5.18858385e+00,  4.99591398e+00, ...,\n",
      "          4.32463980e+00,  4.88260031e+00,  2.15927792e+00],\n",
      "        ...,\n",
      "        [ 2.66906891e+01,  5.97488642e+00,  3.97684884e+00, ...,\n",
      "          1.81616807e+00,  3.17958093e+00,  1.58686340e+00],\n",
      "        [ 2.66891594e+01,  5.97417974e+00,  3.97685862e+00, ...,\n",
      "          1.81650770e+00,  3.17984843e+00,  1.58703721e+00],\n",
      "        [ 2.66876431e+01,  5.97349787e+00,  3.97689652e+00, ...,\n",
      "          1.81687486e+00,  3.18015218e+00,  1.58724236e+00]],\n",
      "\n",
      "       [[ 1.55943689e+01,  3.04951859e+01,  4.52333546e+00, ...,\n",
      "          2.27140903e+00,  2.18136191e+00,  7.17701674e+00],\n",
      "        [ 1.18012750e+00,  8.73062897e+00,  9.33994770e+00, ...,\n",
      "          1.04946575e+01,  1.06560793e+01,  2.48889637e+01],\n",
      "        [ 4.66339254e+00,  7.11968374e+00,  5.22096825e+00, ...,\n",
      "          4.21142054e+00,  4.34452295e+00,  3.45933747e+00],\n",
      "        ...,\n",
      "        [ 2.45418644e+01,  7.24271822e+00,  3.31116414e+00, ...,\n",
      "          2.10865664e+00,  3.07619905e+00,  1.48425937e+00],\n",
      "        [ 2.45362930e+01,  7.25267410e+00,  3.31179953e+00, ...,\n",
      "          2.11027455e+00,  3.07793617e+00,  1.48686242e+00],\n",
      "        [ 2.45307350e+01,  7.26253700e+00,  3.31247163e+00, ...,\n",
      "          2.11194348e+00,  3.07971787e+00,  1.48943877e+00]],\n",
      "\n",
      "       [[ 1.55389032e+01,  3.07419720e+01,  4.70425415e+00, ...,\n",
      "          2.47499228e+00,  2.37119770e+00,  7.33957434e+00],\n",
      "        [ 1.52140152e+00,  9.64664650e+00,  9.31663513e+00, ...,\n",
      "          1.02874870e+01,  1.05597963e+01,  2.42333164e+01],\n",
      "        [ 2.42299691e-01,  4.74217129e+00,  4.00272512e+00, ...,\n",
      "          3.49346423e+00,  3.99866152e+00,  2.86018324e+00],\n",
      "        ...,\n",
      "        [ 2.69070129e+01,  6.95961857e+00,  4.53790808e+00, ...,\n",
      "          1.88002825e+00,  3.36983132e+00,  1.52219903e+00],\n",
      "        [ 2.69056454e+01,  6.95868397e+00,  4.53752232e+00, ...,\n",
      "          1.88047457e+00,  3.36975074e+00,  1.52267814e+00],\n",
      "        [ 2.69041996e+01,  6.95775747e+00,  4.53719330e+00, ...,\n",
      "          1.88099313e+00,  3.36976337e+00,  1.52318525e+00]],\n",
      "\n",
      "       [[ 1.55513792e+01,  3.08214722e+01,  4.74360561e+00, ...,\n",
      "          2.48744464e+00,  2.38796091e+00,  7.37004471e+00],\n",
      "        [ 9.52203274e-01,  9.51055527e+00,  9.59064674e+00, ...,\n",
      "          1.05492458e+01,  1.07822781e+01,  2.41835709e+01],\n",
      "        [-8.02043825e-03,  5.08083105e+00,  4.91119909e+00, ...,\n",
      "          4.21333742e+00,  4.74989653e+00,  3.03363299e+00],\n",
      "        ...,\n",
      "        [ 2.67797527e+01,  6.76668262e+00,  4.36982870e+00, ...,\n",
      "          1.94816160e+00,  3.31227279e+00,  1.63239121e+00],\n",
      "        [ 2.67778416e+01,  6.76635075e+00,  4.36989164e+00, ...,\n",
      "          1.94870746e+00,  3.31260729e+00,  1.63259518e+00],\n",
      "        [ 2.67759647e+01,  6.76601267e+00,  4.36995745e+00, ...,\n",
      "          1.94925821e+00,  3.31294990e+00,  1.63281155e+00]]],\n",
      "      dtype=float32), array([[[-5.1073157e-03, -5.6298338e-03,  2.5003739e-03, ...,\n",
      "          4.7557033e-03, -6.1278499e-04,  6.1892322e-03],\n",
      "        [-2.9886490e-01, -1.7482992e-01, -4.5721136e-02, ...,\n",
      "          4.3498084e-01,  2.1084769e-01,  4.6480930e-01],\n",
      "        [ 4.0640646e-01,  3.9979735e-01, -1.5623994e-01, ...,\n",
      "          5.6744766e-01,  1.5978943e+00,  5.0733215e-01],\n",
      "        ...,\n",
      "        [-3.6136981e-03, -2.7136125e-03, -2.2154527e-03, ...,\n",
      "          2.7486805e-03, -1.1205858e-03,  4.4484981e-03],\n",
      "        [-3.6136981e-03, -2.7136125e-03, -2.2154527e-03, ...,\n",
      "          2.7486805e-03, -1.1205858e-03,  4.4484981e-03],\n",
      "        [-3.6136981e-03, -2.7136125e-03, -2.2154527e-03, ...,\n",
      "          2.7486805e-03, -1.1205858e-03,  4.4484981e-03]],\n",
      "\n",
      "       [[-5.7554320e-03, -6.2729293e-03,  2.3901318e-03, ...,\n",
      "          4.1947146e-03, -2.4449860e-03,  7.0130872e-03],\n",
      "        [-2.1241216e-01, -2.1743007e-02, -8.7041593e-01, ...,\n",
      "          3.9169872e-01,  3.7192538e-01,  3.7351573e-01],\n",
      "        [-4.5019579e-01, -6.1220139e-01, -7.0627190e-02, ...,\n",
      "          1.4114328e-01,  3.2936651e-01,  5.9529686e-01],\n",
      "        ...,\n",
      "        [-4.1964883e-03, -3.2489938e-03, -7.0481159e-04, ...,\n",
      "          3.5837011e-03, -1.4019794e-03,  5.1327576e-03],\n",
      "        [-4.1964883e-03, -3.2489938e-03, -7.0481159e-04, ...,\n",
      "          3.5837011e-03, -1.4019794e-03,  5.1327576e-03],\n",
      "        [-4.1964883e-03, -3.2489938e-03, -7.0481159e-04, ...,\n",
      "          3.5837011e-03, -1.4019794e-03,  5.1327576e-03]],\n",
      "\n",
      "       [[-6.3026347e-03, -6.6007669e-03,  2.0843702e-03, ...,\n",
      "          3.8500598e-03, -2.3301169e-03,  6.9180117e-03],\n",
      "        [ 1.1644986e-03,  6.1298721e-02,  1.0484437e-01, ...,\n",
      "          7.8575039e-01,  6.5836251e-02, -4.9161881e-01],\n",
      "        [-1.4898963e-01, -1.6802147e-02, -3.0694857e-01, ...,\n",
      "          1.0243300e+00, -4.8702970e-01, -1.6638739e-02],\n",
      "        ...,\n",
      "        [-4.5050597e-03, -3.7316557e-03, -1.4314947e-03, ...,\n",
      "          2.6638003e-03, -1.8221233e-03,  4.8167366e-03],\n",
      "        [-4.5050597e-03, -3.7316557e-03, -1.4314947e-03, ...,\n",
      "          2.6638003e-03, -1.8221233e-03,  4.8167366e-03],\n",
      "        [-4.5050597e-03, -3.7316557e-03, -1.4314947e-03, ...,\n",
      "          2.6638003e-03, -1.8221233e-03,  4.8167366e-03]],\n",
      "\n",
      "       [[-4.7542797e-03, -6.0323528e-03,  3.2130189e-03, ...,\n",
      "          5.2997153e-03, -2.0523923e-03,  6.2343772e-03],\n",
      "        [-2.4152602e-01,  3.2704372e-02, -6.1754489e-01, ...,\n",
      "          4.3340051e-01,  2.7746409e-01,  4.5700321e-01],\n",
      "        [-3.6346221e-01, -5.4090077e-01, -1.4890532e-02, ...,\n",
      "          2.7628466e-01,  2.0104563e-01,  6.2872183e-01],\n",
      "        ...,\n",
      "        [-9.9796839e-02, -2.4710448e-02, -2.0853123e-02, ...,\n",
      "          2.1087807e-01,  9.3531534e-02,  2.8427148e-01],\n",
      "        [-2.6674977e-01, -9.7112551e-02, -8.5148060e-01, ...,\n",
      "         -2.9130608e-01,  1.7988671e-01,  3.6495960e-01],\n",
      "        [ 1.1719018e-03, -8.9455768e-04, -8.6635109e-03, ...,\n",
      "          5.0267600e-03, -3.1892632e-03,  4.7088368e-03]],\n",
      "\n",
      "       [[-4.4687800e-03, -6.4317617e-03,  3.0625132e-03, ...,\n",
      "          4.9755070e-03, -1.9300299e-03,  6.7157205e-03],\n",
      "        [-9.0723768e-02,  8.7390447e-01,  2.0420898e-01, ...,\n",
      "          3.9543021e-01, -8.6047448e-02,  4.2815030e-01],\n",
      "        [-2.4393776e-01, -4.7558179e-01,  1.9032761e-02, ...,\n",
      "          3.8917854e-01,  3.6162105e-01,  8.9818807e-03],\n",
      "        ...,\n",
      "        [-2.9152541e-03, -2.7986406e-03, -3.3334154e-03, ...,\n",
      "          3.5997906e-03, -7.3503924e-04,  4.6348143e-03],\n",
      "        [-2.9152541e-03, -2.7986406e-03, -3.3334154e-03, ...,\n",
      "          3.5997906e-03, -7.3503924e-04,  4.6348143e-03],\n",
      "        [-2.9152541e-03, -2.7986406e-03, -3.3334154e-03, ...,\n",
      "          3.5997906e-03, -7.3503924e-04,  4.6348143e-03]]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "print(type(outputs.predictions))  # 데이터 타입\n",
    "print(outputs.predictions)       # 데이터 내용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2095158b-918b-4640-bef2-47229536d557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplant', 'CUIDlgImplantLib._createImage() CUIDlgImplantLib._createImage() CUIDlgImplantLib._createImage() CUIDlgImplantLib._createImage() CUIDlgImplantLib._createImage() CUIDlgImplantLib._createImage() CUIDlgImplantLib._createImage() CUIDlgImplantLib._createImage() CUIDlgImplantLib._createImage() CUIDlgImplantLib._', 'BEGIN_MESSAGE_MAP(CUIDlgImplantLib, CUIDialog) BEGIN_MESSAGE_MAP(CUIDlgImplantLib, CUIDialog) END_MESSAGE_MAP(CUIDlgImplantLib, CUIDialog) BEGIN_MESSAGE_MAP(CUIDlgImplantLib, CUIDialog) END_MESSAGE_MAP(CUIDlgImplantLib, CUIDialog) END_MESSAGE_MAP(CUIDlgImplantLib', 'CUIDlgImplantLib.OnLButtonDown(UINT, CPoint).CUIDlgImplantLib.OnLButtonDown(UINT, CPoint).CUIDlgImplantLib.OnLButtonDown(UINT, CPoint).CUIDlgImplantLib.OnLButtonDown(UINT, CPoint).CUIDlgImplantLib.OnLButtonDown(UINT, CPoint).CUIDlgImplantLib.', 'CUIDlgImplantLib.OnEraseBkgnd(CDC*pDC).CUIDlgImplantLib.OnEraseBkgnd(CDC*pDC).CUIDlgImplantLib.OnEraseBkgnd(CDC*pDC).CUIDlgImplantLib.OnEraseBkgnd(CDC*pDC).CUIDlgImplantLib.OnEraseBkgn']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 데이터셋에서 입력 텍스트 가져오기\n",
    "test_input_ids = test_1_datas[\"test\"][\"input_ids\"]\n",
    "test_attention_mask = test_1_datas[\"test\"][\"attention_mask\"]\n",
    "\n",
    "# PyTorch 텐서로 변환\n",
    "input_ids = torch.tensor(test_input_ids).to(model.device)\n",
    "attention_mask = torch.tensor(test_attention_mask).to(model.device)\n",
    "\n",
    "# 예측 토큰 ID 생성\n",
    "generated_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=128, num_beams=5)\n",
    "\n",
    "# 디코딩하여 텍스트 변환\n",
    "predictions = [tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids]\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "59175583-4f93-42be-bad7-f90e08498449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.trainer_utils.PredictionOutput'>\n",
      "label ids\n",
      "<class 'numpy.ndarray'> \t (5, 512)\n",
      "predictions\n",
      "<class 'tuple'> \t [\"<class 'numpy.ndarray'> (5, 512, 32100)\", \"<class 'numpy.ndarray'> (5, 258, 512)\"]\n"
     ]
    }
   ],
   "source": [
    "#평가 시 생성된 결과 출력\n",
    "outputs = trainer.predict(test_dataset=test_1_datas[\"test\"])\n",
    "# print(outputs.predictions)\n",
    "\n",
    "print(type(outputs))\n",
    "print('label ids')\n",
    "print(type(outputs.label_ids), \"\\t\", outputs.label_ids.shape)\n",
    "\n",
    "print('\\npredictions')\n",
    "pr_types = [f'{type(d)} {d.shape}' for d in outputs.predictions]\n",
    "print(type(outputs.predictions), \"\\t\", pr_types)\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# # 로짓에서 가장 높은 확률의 토큰 선택\n",
    "# logits = outputs.predictions  # (batch_size, sequence_length, vocab_size)\n",
    "# token_ids = np.argmax(logits, axis=-1)  # 각 위치에서 확률이 가장 높은 토큰 ID 선택\n",
    "\n",
    "# # 디코딩\n",
    "# predictions = [tokenizer.decode(ids, skip_special_tokens=True) for ids in token_ids]\n",
    "# print(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a7c3e76e-bf27-4ac6-8dd8-6eee2116fed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method RobertaTokenizer.get_vocab of RobertaTokenizer(name_or_path='Salesforce/codet5-small', vocab_size=32100, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>', 'additional_special_tokens': ['<extra_id_99>', '<extra_id_98>', '<extra_id_97>', '<extra_id_96>', '<extra_id_95>', '<extra_id_94>', '<extra_id_93>', '<extra_id_92>', '<extra_id_91>', '<extra_id_90>', '<extra_id_89>', '<extra_id_88>', '<extra_id_87>', '<extra_id_86>', '<extra_id_85>', '<extra_id_84>', '<extra_id_83>', '<extra_id_82>', '<extra_id_81>', '<extra_id_80>', '<extra_id_79>', '<extra_id_78>', '<extra_id_77>', '<extra_id_76>', '<extra_id_75>', '<extra_id_74>', '<extra_id_73>', '<extra_id_72>', '<extra_id_71>', '<extra_id_70>', '<extra_id_69>', '<extra_id_68>', '<extra_id_67>', '<extra_id_66>', '<extra_id_65>', '<extra_id_64>', '<extra_id_63>', '<extra_id_62>', '<extra_id_61>', '<extra_id_60>', '<extra_id_59>', '<extra_id_58>', '<extra_id_57>', '<extra_id_56>', '<extra_id_55>', '<extra_id_54>', '<extra_id_53>', '<extra_id_52>', '<extra_id_51>', '<extra_id_50>', '<extra_id_49>', '<extra_id_48>', '<extra_id_47>', '<extra_id_46>', '<extra_id_45>', '<extra_id_44>', '<extra_id_43>', '<extra_id_42>', '<extra_id_41>', '<extra_id_40>', '<extra_id_39>', '<extra_id_38>', '<extra_id_37>', '<extra_id_36>', '<extra_id_35>', '<extra_id_34>', '<extra_id_33>', '<extra_id_32>', '<extra_id_31>', '<extra_id_30>', '<extra_id_29>', '<extra_id_28>', '<extra_id_27>', '<extra_id_26>', '<extra_id_25>', '<extra_id_24>', '<extra_id_23>', '<extra_id_22>', '<extra_id_21>', '<extra_id_20>', '<extra_id_19>', '<extra_id_18>', '<extra_id_17>', '<extra_id_16>', '<extra_id_15>', '<extra_id_14>', '<extra_id_13>', '<extra_id_12>', '<extra_id_11>', '<extra_id_10>', '<extra_id_9>', '<extra_id_8>', '<extra_id_7>', '<extra_id_6>', '<extra_id_5>', '<extra_id_4>', '<extra_id_3>', '<extra_id_2>', '<extra_id_1>', '<extra_id_0>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32000: AddedToken(\"<extra_id_99>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32001: AddedToken(\"<extra_id_98>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32002: AddedToken(\"<extra_id_97>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32003: AddedToken(\"<extra_id_96>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32004: AddedToken(\"<extra_id_95>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32005: AddedToken(\"<extra_id_94>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32006: AddedToken(\"<extra_id_93>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32007: AddedToken(\"<extra_id_92>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32008: AddedToken(\"<extra_id_91>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32009: AddedToken(\"<extra_id_90>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32010: AddedToken(\"<extra_id_89>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32011: AddedToken(\"<extra_id_88>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32012: AddedToken(\"<extra_id_87>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32013: AddedToken(\"<extra_id_86>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32014: AddedToken(\"<extra_id_85>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32015: AddedToken(\"<extra_id_84>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32016: AddedToken(\"<extra_id_83>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32017: AddedToken(\"<extra_id_82>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32018: AddedToken(\"<extra_id_81>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32019: AddedToken(\"<extra_id_80>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32020: AddedToken(\"<extra_id_79>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32021: AddedToken(\"<extra_id_78>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32022: AddedToken(\"<extra_id_77>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32023: AddedToken(\"<extra_id_76>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32024: AddedToken(\"<extra_id_75>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32025: AddedToken(\"<extra_id_74>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32026: AddedToken(\"<extra_id_73>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32027: AddedToken(\"<extra_id_72>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32028: AddedToken(\"<extra_id_71>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32029: AddedToken(\"<extra_id_70>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32030: AddedToken(\"<extra_id_69>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32031: AddedToken(\"<extra_id_68>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32032: AddedToken(\"<extra_id_67>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32033: AddedToken(\"<extra_id_66>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32034: AddedToken(\"<extra_id_65>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32035: AddedToken(\"<extra_id_64>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32036: AddedToken(\"<extra_id_63>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32037: AddedToken(\"<extra_id_62>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32038: AddedToken(\"<extra_id_61>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32039: AddedToken(\"<extra_id_60>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32040: AddedToken(\"<extra_id_59>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32041: AddedToken(\"<extra_id_58>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32042: AddedToken(\"<extra_id_57>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32043: AddedToken(\"<extra_id_56>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32044: AddedToken(\"<extra_id_55>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32045: AddedToken(\"<extra_id_54>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32046: AddedToken(\"<extra_id_53>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32047: AddedToken(\"<extra_id_52>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32048: AddedToken(\"<extra_id_51>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32049: AddedToken(\"<extra_id_50>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32050: AddedToken(\"<extra_id_49>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32051: AddedToken(\"<extra_id_48>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32052: AddedToken(\"<extra_id_47>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32053: AddedToken(\"<extra_id_46>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32054: AddedToken(\"<extra_id_45>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32055: AddedToken(\"<extra_id_44>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32056: AddedToken(\"<extra_id_43>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32057: AddedToken(\"<extra_id_42>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32058: AddedToken(\"<extra_id_41>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32059: AddedToken(\"<extra_id_40>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32060: AddedToken(\"<extra_id_39>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32061: AddedToken(\"<extra_id_38>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32062: AddedToken(\"<extra_id_37>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32063: AddedToken(\"<extra_id_36>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32064: AddedToken(\"<extra_id_35>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32065: AddedToken(\"<extra_id_34>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32066: AddedToken(\"<extra_id_33>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32067: AddedToken(\"<extra_id_32>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32068: AddedToken(\"<extra_id_31>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32069: AddedToken(\"<extra_id_30>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32070: AddedToken(\"<extra_id_29>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32071: AddedToken(\"<extra_id_28>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32072: AddedToken(\"<extra_id_27>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32073: AddedToken(\"<extra_id_26>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32074: AddedToken(\"<extra_id_25>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32075: AddedToken(\"<extra_id_24>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32076: AddedToken(\"<extra_id_23>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32077: AddedToken(\"<extra_id_22>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32078: AddedToken(\"<extra_id_21>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32079: AddedToken(\"<extra_id_20>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32080: AddedToken(\"<extra_id_19>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32081: AddedToken(\"<extra_id_18>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32082: AddedToken(\"<extra_id_17>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32083: AddedToken(\"<extra_id_16>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32084: AddedToken(\"<extra_id_15>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32085: AddedToken(\"<extra_id_14>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32086: AddedToken(\"<extra_id_13>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32087: AddedToken(\"<extra_id_12>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32088: AddedToken(\"<extra_id_11>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32089: AddedToken(\"<extra_id_10>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32090: AddedToken(\"<extra_id_9>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32091: AddedToken(\"<extra_id_8>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32092: AddedToken(\"<extra_id_7>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32093: AddedToken(\"<extra_id_6>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32094: AddedToken(\"<extra_id_5>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32095: AddedToken(\"<extra_id_4>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32096: AddedToken(\"<extra_id_3>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32097: AddedToken(\"<extra_id_2>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32098: AddedToken(\"<extra_id_1>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t32099: AddedToken(\"<extra_id_0>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "}>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "63b026a6-9dac-48c2-8fc5-dd330673f9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([128])\n",
      "Generated Output: <pad><s><extra_id_0>CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplant\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<pad><s><extra_id_0>CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplantLib.~CUIDlgImplant'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code ='''\n",
    "BOOL CUIDlgImplantLib::OnInitDialog()\n",
    "{\n",
    "\tCToolbarModuleBase::OnInitDialog();\n",
    "\n",
    "\t// TODO:  여기에 추가 초기화 작업을 추가합니다.\n",
    "\t\n",
    "\t_createImplantLibraryDlg();\n",
    "\t_createGroupDlg();\n",
    "\n",
    "\tCreateSlider();\n",
    "\n",
    "\t_pModuleDocAPImplantSimulation = reinterpret_cast<CModuleDocAPImplantSimulation*>(_pLocalDoc->GetWritableModuleDoc(APMod_ImplantSimul));\n",
    "\t_pModuleDocAPImplantSimulation->CheckImplantXml();\n",
    "\n",
    "\tInitializeToothBtnFlags();\n",
    "\n",
    "\tShowUndoRedoWindow(TRUE);\n",
    "\n",
    "\treturn TRUE;  // return TRUE unless you set the focus to a control\n",
    "\t\t\t\t  // 예외: OCX 속성 페이지는 FALSE를 반환해야 합니다.\n",
    "}\n",
    "'''\n",
    "\n",
    "# 토크나이징\n",
    "input_ids = tokenizer(code, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128).input_ids\n",
    "\n",
    "#디코딩\n",
    "output_text = tokenizer.decode(generated_ids[0])\n",
    "\n",
    "print(type(generated_ids[0]), generated_ids[0].shape)\n",
    "print(\"Generated Output:\", output_text)\n",
    "output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "63251107-5e70-4f6a-b4d1-a89db55fd5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: {'<pad>'}\n",
      "1: {'<s>'}\n",
      "32099: {'<extra_id_0>'}\n",
      "39: {'C'}\n",
      "3060: {'UID'}\n",
      "23623: {'lg'}\n",
      "2828: {'Impl'}\n",
      "970: {'ant'}\n",
      "5664: {'Lib'}\n",
      "18: {'.'}\n",
      "98: {'~'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokens = generated_ids[0]\n",
    "check_token_map = {}\n",
    "\n",
    "for tk in tokens:\n",
    "    check_token_map[int(tk)] = tokenizer.decode(tk)\n",
    "\n",
    "for key, data in check_token_map.items():\n",
    "    print(f'{key}:', {data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9b26298e-859d-463c-a1ae-2525c740f478",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (512,)\n",
      "1: {'<s>'}\n",
      "39: {'C'}\n",
      "3060: {'UID'}\n",
      "23623: {'lg'}\n",
      "2828: {'Impl'}\n",
      "970: {'ant'}\n",
      "5664: {'Lib'}\n",
      "385: {' C'}\n",
      "18: {'.'}\n",
      "98: {'~'}\n",
      "1435: {'()'}\n",
      "7010: {' \\n'}\n",
      "203: {'\\n'}\n",
      "6315: {'._'}\n",
      "84: {'p'}\n",
      "12804: {'Img'}\n",
      "4247: {'Title'}\n",
      "682: {'List'}\n",
      "5669: {'Man'}\n",
      "11853: {'ufact'}\n",
      "594: {'ure'}\n",
      "225: {' '}\n",
      "3163: {'System'}\n",
      "5895: {'Ab'}\n",
      "322: {'ut'}\n",
      "475: {'ment'}\n",
      "1488: {'Model'}\n",
      "40: {'D'}\n",
      "11124: {'Preview'}\n",
      "2: {'</s>'}\n",
      "0: {'<pad>'}\n"
     ]
    }
   ],
   "source": [
    "print(type(outputs.label_ids[0]), outputs.label_ids[0].shape)\n",
    "for tokens in outputs.label_ids:\n",
    "    # print(tokens)\n",
    "    \n",
    "    # print(tokenizer.decode(tokens))\n",
    "    break\n",
    "\n",
    "\n",
    "tokens = outputs.label_ids[0]\n",
    "check_token_map = {}\n",
    "for tk in tokens:\n",
    "    check_token_map[int(tk)] = tokenizer.decode(tk)\n",
    "\n",
    "for key, data in check_token_map.items():\n",
    "    print(f'{key}:', {data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c9949cd-478e-409a-b300-9762eb4b4919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "source": [
    "train_data_dict_list = []\n",
    "for idx, info in enumerate(short_method_list):\n",
    "    data_dict = {}\n",
    "    data_dict['oms'] = info\n",
    "    data_dict['code'] = all_code_list[idx]\n",
    "    data_dict['\n",
    "    '] = all_tokens_list[idx]\n",
    "    data_dict['tokens_ids'] = all_tokens_ids_list[idx]\n",
    "    train_data_dict_list.append(data_dict)\n",
    "print(len(train_data_dict_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "778869ae-93e6-46fc-b4c7-57ac8205a3af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n",
      "15 tokens 1 num\n",
      "17 tokens 1 num\n",
      "23 tokens 1 num\n",
      "25 tokens 1 num\n",
      "26 tokens 1 num\n",
      "28 tokens 1 num\n",
      "29 tokens 3 num\n",
      "31 tokens 1 num\n",
      "34 tokens 2 num\n",
      "35 tokens 2 num\n",
      "36 tokens 1 num\n",
      "37 tokens 1 num\n",
      "38 tokens 1 num\n",
      "39 tokens 1 num\n",
      "49 tokens 1 num\n",
      "53 tokens 1 num\n",
      "57 tokens 4 num\n",
      "59 tokens 2 num\n",
      "62 tokens 1 num\n",
      "68 tokens 1 num\n",
      "72 tokens 1 num\n",
      "80 tokens 1 num\n",
      "83 tokens 1 num\n",
      "89 tokens 1 num\n",
      "90 tokens 1 num\n",
      "92 tokens 1 num\n",
      "95 tokens 1 num\n",
      "100 tokens 1 num\n",
      "102 tokens 1 num\n",
      "109 tokens 1 num\n",
      "129 tokens 1 num\n",
      "131 tokens 1 num\n",
      "139 tokens 2 num\n",
      "146 tokens 1 num\n",
      "150 tokens 1 num\n",
      "186 tokens 1 num\n",
      "210 tokens 1 num\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "BERT_MAX_SIZE = 4098\n",
    "token_ids = []\n",
    "token_size_map = defaultdict(list)\n",
    "max_size = 0\n",
    "for data in train_data_dict_list:\n",
    "    token_ids.append(data['tokens_ids'])\n",
    "    size = len(data['tokens_ids'])\n",
    "    token_size_map[size].append(data)\n",
    "    if max_size < size:\n",
    "        max_size = size\n",
    "\n",
    "print(max_size)\n",
    "for size_key, datas in sorted(token_size_map.items()):\n",
    "    print(f'{size_key} tokens {len(datas)} num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e895b194-1c83-4404-9874-68cde14fb9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8edea8-f4ca-43d7-9240-bc059ddb7f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
